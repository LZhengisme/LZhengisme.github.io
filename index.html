<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Lin Zheng</title>
  <meta name="author" content="Lin Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon.webp" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <div class="container">
    <header>
      <div class="profile-photo">
        <a href="images/2023-nov-linzheng.jpg">
          <img src="images/2023-nov-linzheng.jpg" alt="Lin Zheng">
        </a>
      </div>
      <div class="profile-info">
        <h1 class="name">Lin Zheng</h1>
        <div class="links">
          <a href="mailto:linzheng@connect.hku.hk">Email</a>
          <span>|</span>
          <a href="data/cv_linzheng_feb2026.pdf">CV</a>
          <span>|</span>
          <a href="https://scholar.google.com/citations?user=3NXH0t8AAAAJ&hl=en">Scholar</a>
          <span>|</span>
          <a href="https://x.com/linzhengisme">X</a>
          <span>|</span>
          <a href="https://github.com/LZhengisme">Github</a>
        </div>
      </div>
    </header>

    <section class="bio">
      <p>
        I am a fifth-year Ph.D. student in the <a href="https://www.cs.hku.hk/">Department of Computer Science</a> at <a href="https://www.hku.hk/">the University of Hong Kong (HKU)</a>, supervised by <a href="https://ikekonglp.github.io/">Lingpeng Kong</a>. I previously interned at <a href="https://deepmind.google/">Google DeepMind</a>, <a href="https://www.bytedance.com/en">ByteDance</a>, and <a href="https://sambanova.ai/">SambaNova Systems</a>. In 2021, I received my Bachelor degree in the School of Computer Science and Engineering, <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>, working with <a href="https://scholar.google.com/citations?hl=en&user=cuIweygAAAAJ&view_op=list_works&sortby=pubdate">Qinliang Su</a>.
      </p>
    </section>

    <section class="research">
      <h2>Research</h2>
      <div class="research-intro">
        <p>I develop efficient and flexible sequence models. Recently, I've focused on how language models represent inputs beyond tokenization, and how they generate outputs beyond left-to-right autoregression.</p>
        <p class="scholar-note">For a complete list, see <a href="https://scholar.google.com/citations?user=3NXH0t8AAAAJ&hl=en">Google Scholar</a>.</p>
        <p class="scholar-note">* indicates equal contribution.</p>
      </div>

      <h3 class="theme-heading">Beyond Tokenization</h3>
      
      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2602.04289">Proxy Compression for Language Modeling</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>*, <a href="https://scholar.google.com/citations?user=1rcF1toAAAAJ&hl=en">Xinyu Li</a>*, <a href="https://siviltaram.github.io/">Qian Liu</a>, <a href="https://xcfeng.net/">Xiachong Feng</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue">2026</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2602.04289">paper</a> <span>/</span> <a href="https://github.com/LZhengisme/proxy-compression">code</a></div>
      </div>

      <div class="publication">
        <div class="pub-title"><a href="https://hkunlp.github.io/blog/2025/evabyte">EvaByte: Efficient Byte-level Language Models at Scale</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, <a href="https://scholar.google.com/citations?user=h-87C9cAAAAJ&hl">Xueliang Zhao</a>, <a href="https://scholar.google.com/citations?user=ga5aRGkAAAAJ&hl=en">Guangtao Wang</a>, <a href="https://scholar.google.com/citations?user=ZvGfyVUAAAAJ&hl=en">Chen Wu</a>, <a href="https://www.linkedin.com/in/haocheng-dong/">David Dong</a>, <a href="https://www.linkedin.com/in/ke-wang-025279122/">Angela Wang</a>, <a href="https://www.linkedin.com/in/mingran-wang-43973231/">Mingran Wang</a>, <a href="https://www.linkedin.com/in/yun-du-ba99b524/">Yun Du</a>, <a href="https://www.linkedin.com/in/haige-bo-965a82137/">Haige Bo</a>, <a href="https://www.linkedin.com/in/amols2/">Amol Sharma</a>, <a href="https://scholar.google.com/citations?hl=en&user=s4bGwOAAAAAJ">Bo Li</a>, <a href="https://scholar.google.com/citations?user=0q3lUnAAAAAJ&hl=en">Kejie Zhang</a>, <a href="https://scholar.google.com/citations?user=gba_0WoAAAAJ&hl=en">Changran Hu</a>, <a href="https://urmish.github.io/">Urmish Thakker</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue">2025</div>
        <div class="pub-links"><a href="https://hkunlp.github.io/blog/2025/evabyte">blog</a> <span>/</span> <a href="https://github.com/OpenEvaByte/evabyte">code</a></div>
      </div>

      <h3 class="theme-heading">Beyond Autoregressive Models</h3>

      <div class="publication">
        <div class="pub-title"><a href="https://hkunlp.github.io/blog/2025/dreamon">DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-Size Canvas</a></div>
        <div class="pub-authors"><a href="https://williamzr.github.io/">Zirui Wu</a>*, <strong>Lin Zheng</strong>*, <a href="https://zhxie.site">Zhihui Xie</a>, <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>, <a href="https://sites.google.com/site/ysfeng/home">Yansong Feng</a>, <a href="https://zhenguol.github.io/">Zhenguo Li</a>, Victoria W., <a href="https://scholar.google.com/citations?user=n_E0Bg4AAAAJ&hl=en">Guorui Zhou</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICLR</em>, 2026</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2602.01326">paper</a> <span>/</span> <a href="https://hkunlp.github.io/blog/2025/dreamon">blog</a> <span>/</span> <a href="https://github.com/DreamLM/DreamOn">code</a></div>
      </div>

      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2509.01142">Dream-Coder 7B</a></div>
        <div class="pub-authors"><a href="https://zhxie.site">Zhihui Xie</a>*, <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>*, <strong>Lin Zheng</strong>*, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>*, Jingwei Dong, <a href="https://williamzr.github.io/">Zirui Wu</a>, <a href="https://scholar.google.com/citations?user=h-87C9cAAAAJ&hl">Xueliang Zhao</a>, <a href="https://summmeer.github.io/">Shansan Gong</a>, <a href="https://scholar.google.com/citations?hl=en&user=DUfcez0AAAAJ">Xin Jiang</a>, <a href="https://zhenguol.github.io/">Zhenguo Li</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue">2025</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2509.01142">paper</a> <span>/</span> <a href="https://hkunlp.github.io/blog/2025/dream-coder">blog</a> <span>/</span> <a href="https://github.com/DreamLM/Dream-Coder">code</a></div>
      </div>

      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2508.15487">Dream 7B: Diffusion Large Language Models</a></div>
        <div class="pub-authors"><a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>*, <a href="https://zhxie.site">Zhihui Xie</a>*, <strong>Lin Zheng</strong>*, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>*, <a href="https://williamzr.github.io/">Zirui Wu</a>, <a href="https://scholar.google.com/citations?hl=en&user=DUfcez0AAAAJ">Xin Jiang</a>, <a href="https://zhenguol.github.io/">Zhenguo Li</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue">2025</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2508.15487">paper</a> <span>/</span> <a href="https://hkunlp.github.io/blog/2025/dream">blog</a> <span>/</span> <a href="https://github.com/HKUNLP/Dream">code</a></div>
      </div>

      <!-- <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/abs/2402.07754">Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</a></div>
        <div class="pub-authors"><a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>, <a href="https://summmeer.github.io/">Shansan Gong</a>, <strong>Lin Zheng</strong>, <a href="https://scholar.google.com/citations?hl=en&user=DUfcez0AAAAJ">Xin Jiang</a>, <a href="https://zhenguol.github.io/">Zhenguo Li</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICLR</em>, 2025</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2410.14157">paper</a> <span>/</span> <a href="https://github.com/HKUNLP/diffusion-vs-ar">code</a></div>
      </div> -->

      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/abs/2402.07754">Scaling Diffusion Language Models via Adaptation from Autoregressive Models</a></div>
        <div class="pub-authors"><a href="https://summmeer.github.io/">Shansan Gong</a>*, <a href="https://shivamag125.github.io/">Shivam Agarwal</a>*, <a href="https://dreasysnail.github.io/">Yizhe Zhang</a>, <a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>, <strong>Lin Zheng</strong>, <a href="https://scholar.google.com/citations?user=BizedOAAAAAJ">Mukai Li</a>, <a href="https://chenxinan-fdu.github.io/">Chenxin An</a>, <a href="https://peilinzhao.github.io/">Peilin Zhao</a>, <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ">Wei Bi</a>, <a href="https://haopeng-nlp.github.io/">Hao Peng</a>, <a href="https://hanj.cs.illinois.edu/">Jiawei Han</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICLR</em>, 2025</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2410.17891">paper</a> <span>/</span> <a href="https://github.com/HKUNLP/DiffuLLaMA">code</a></div>
      </div>

      <!-- <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/abs/2402.07754">Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models</a></div>
        <div class="pub-authors"><a href="https://jiacheng-ye.github.io/">Jiacheng Ye</a>*, <a href="https://summmeer.github.io/">Shansan Gong</a>*, <a href="https://scholar.google.com/citations?user=MX0S_1IAAAAJ&hl=en">Liheng Chen</a>*, <strong>Lin Zheng</strong>, <a href="https://sumilergao.github.io/jiahuig.hku/">Jiahui Gao</a>, <a href="https://han-shi.github.io/">Han Shi</a>, <a href="https://i.cs.hku.hk/~cwu/">Chuan Wu</a>, <a href="https://zhenguol.github.io/">Zhenguo Li</a>, <a href="https://scholar.google.com/citations?user=aSJcgQMAAAAJ">Wei Bi</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>NeurIPS</em>, 2024</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2402.07754.pdf">paper</a> <span>/</span> <a href="https://github.com/HKUNLP/diffusion-of-thoughts">code</a></div>
      </div> -->

      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2311.17972.pdf">Self-Infilling Code Generation</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Jianbo Yuan</a>, <a href="https://zhreshold.github.io/">Zhi Zhang</a>, <a href="https://sites.google.com/site/hystatistics/home">Hongxia Yang</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICML</em>, 2024</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2311.17972.pdf">paper</a> <span>/</span> <a href="https://github.com/LZhengisme/self-infilling">code</a></div>
      </div>

      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2302.05737.pdf">A Reparameterized Discrete Diffusion Model for Text Generation</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Jianbo Yuan</a>, <a href="https://www.cs.ox.ac.uk/people/lei.yu/">Lei Yu</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>COLM</em>, 2024</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2302.05737.pdf">paper</a> <span>/</span> <a href="https://github.com/HKUNLP/reparam-discrete-diffusion">code</a></div>
      </div>

      <h3 class="theme-heading">Model Architectures</h3>

      <div class="publication">
        <div class="pub-title"><a href="https://openreview.net/forum?id=G-uNfHKrj46">Efficient Attention via Control Variates</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Jianbo Yuan</a>, <a href="https://chongw.github.io">Chong Wang</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICLR</em>, 2023 <span class="oral">(oral)</span></div>
        <div class="pub-links"><a href="https://openreview.net/forum?id=G-uNfHKrj46">paper</a> <span>/</span> <a href="https://github.com/LZhengisme/efficient-attention">code</a></div>
      </div>

      <!-- <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2210.07661.pdf">CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling</a></div>
        <div class="pub-authors"><a href="https://scholar.google.com/citations?user=19qq4hsAAAAJ">Jun Zhang</a>, <a href="https://scholar.google.com/citations?user=slwTiOUAAAAJ">Shuyang Jiang</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <strong>Lin Zheng</strong>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICML</em>, 2023</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2210.07661.pdf">paper</a> <span>/</span> <a href="https://github.com/Shark-NLP/CAB">code</a></div>
      </div> -->

      <!-- <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/abs/2312.11135">Linear Attention via Orthogonal Memory</a></div>
        <div class="pub-authors"><a href="https://scholar.google.com/citations?user=19qq4hsAAAAJ">Jun Zhang</a>, <a href="https://scholar.google.com/citations?user=slwTiOUAAAAJ">Shuyang Jiang</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <strong>Lin Zheng</strong>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>arXiv preprint</em>, 2023</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2312.11135.pdf">paper</a></div>
      </div> -->

      <!-- <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2310.09512.pdf">Attentive Multi-Layer Perceptron for Non-autoregressive Generation</a></div>
        <div class="pub-authors"><a href="https://scholar.google.com/citations?user=slwTiOUAAAAJ">Shuyang Jiang</a>, <a href="https://scholar.google.com/citations?user=19qq4hsAAAAJ">Jun Zhang</a>, <a href="https://jiangtaofeng.github.io/">Jiangtao Feng</a>, <strong>Lin Zheng</strong>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ECML/PKDD</em>, 2023</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2310.09512.pdf">paper</a> <span>/</span> <a href="https://github.com/Shark-NLP/AttentiveMLP">code</a></div>
      </div> -->

      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2204.04667.pdf">Linear Complexity Randomized Self-attention Mechanism</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, <a href="https://chongw.github.io">Chong Wang</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICML</em>, 2022</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2204.04667.pdf">paper</a> <span>/</span> <a href="https://github.com/LZhengisme/efficient-attention">code</a></div>
      </div>

      <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/pdf/2110.02453.pdf">Ripple Attention for Visual Perception with Sub-quadratic Complexity</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, Huijie Pan, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ICML</em>, 2022</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2110.02453.pdf">paper</a></div>
      </div>

      <!-- <div class="publication">
        <div class="pub-title"><a href="https://aclanthology.org/2021.acl-long.45.pdf">Cascaded Head-colliding Attention</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>ACL</em>, 2021</div>
        <div class="pub-links"><a href="https://aclanthology.org/2021.acl-long.45.pdf">paper</a> <span>/</span> <a href="https://github.com/LZhengisme/CODA">code</a></div>
      </div> -->

      <!-- <div class="publication">
        <div class="pub-title"><a href="https://arxiv.org/abs/2302.12563">Retrieved Sequence Augmentation for Protein Representation Learning</a></div>
        <div class="pub-authors"><a href="https://chang-github-00.github.io/-changma/">Chang Ma</a>, <a href="https://zhao-ht.github.io/haitengzhao/">Haiteng Zhao</a>, <strong>Lin Zheng</strong>, <a href="https://scholar.google.com/citations?user=i2Zn_I0AAAAJ">Jiayi Xin</a>, <a href="https://qtli.github.io/">Qintong Li</a>, Lijun Wu, Zhihong Deng, Yang Lu, <a href="https://leuchine.github.io/">Qi Liu</a>, <a href="https://ikekonglp.github.io/">Lingpeng Kong</a></div>
        <div class="pub-venue"><em>arXiv preprint</em>, 2023</div>
        <div class="pub-links"><a href="https://arxiv.org/pdf/2302.12563.pdf">paper</a> <span>/</span> <a href="https://github.com/HKUNLP/RSA">code</a></div>
      </div> -->
      <!-- <div class="publication">          
        <div class="pub-title"><a href="https://www.aclweb.org/anthology/2020.acl-main.71.pdf">Generative Semantic Hashing Enhanced via Boltzmann Machines</a></div>
        <div class="pub-authors"><strong>Lin Zheng</strong>, <a href="https://cse.sysu.edu.cn/content/3796">Qinliang Su</a>, <a href="https://sites.google.com/view/dinghanshen">Dinghan Shen</a>, <a href="https://cse.buffalo.edu/~changyou/">Changyou Chen</a></div>
        <div class="pub-venue"><em>ACL</em>, 2020</div>
        <div class="pub-links"><a href="https://www.aclweb.org/anthology/2020.acl-main.71.pdf">paper</a> <span>/</span> <a href="https://github.com/LZhengisme/CorrelatedSemanticHashing">code</a></div>
      </div> -->

    </section>

    <section class="compact-section">
      <h2>Teaching</h2>
      <p>Spring 2022 and Spring 2023: TA for COMP3314B Machine Learning</p>
    </section>

    <section class="compact-section">
      <h2>Service</h2>
      <p>Reviewer: ACL 2021, NAACL 2021, ICML 2022-2024, NeurIPS 2022-2023, etc.</p>
    </section>

    <footer>
      <p>Source code from <a href="https://jonbarron.info/">Jon Barron's</a> website.</p>
    </footer>
  </div>
</body>
</html>
