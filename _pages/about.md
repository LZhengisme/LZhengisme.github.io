---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


# About me
I am a second-year Ph.D. student at the Department of Computer Science in the University of Hong Kong (HKU), supervised by [Lingpeng Kong](https://ikekonglp.github.io/), [Ping Luo](http://luoping.me/), and [Ben Kao](https://www.cs.hku.hk/people/academic-staff/kao). Recently, I received my Bachelor degree in the School of Computer Science and Engineering, [Sun Yat-sen University](http://www.sysu.edu.cn/en/index.htm), working with [Qinliang Su](https://scholar.google.com/citations?hl=en&user=cuIweygAAAAJ&view_op=list_works&sortby=pubdate). My research interests include machine learning and probabilistic graphical models.

<!-- ## News -->

## Preprints

- A Reparameterized Discrete Diffusion Model for Text Generation <a href="https://arxiv.org/pdf/2302.05737.pdf">[pdf]</a> <a href="https://github.com/HKUNLP/reparam-discrete-diffusion">[code]</a> <br>
  <b>Lin Zheng</b>, Jianbo Yuan, Lei Yu, and Lingpeng Kong <br>
  arXiv:2302.05737, 2023

- CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling <a href="https://arxiv.org/pdf/2210.07661.pdf">[pdf]</a> <a href="https://github.com/Shark-NLP/CAB">[code]</a> <br>
  Jun Zhang, Shuyang Jiang, Jiangtao Feng, <b>Lin Zheng</b>, and Lingpeng Kong <br>
  arXiv:2210.07661, 2022
  

## Publications

- Efficient Attention via Control Variates <a href="https://openreview.net/forum?id=G-uNfHKrj46">[pdf]</a> <a href="https://github.com/LZhengisme/efficient-attention">[code]</a> <br> 
  <b>Lin Zheng</b>, <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Jianbo Yuan</a>, <a href="https://chongw.github.io">Chong Wang</a>, and <a href="https://ikekonglp.github.io/">Lingpeng Kong</a> <br>
  In International Conference on Learning Representations (ICLR), 2023 (<b>oral</b>) 

- Linear Complexity Randomized Self-attention Mechanism <a href="https://arxiv.org/pdf/2204.04667.pdf">[pdf]</a> <a href="https://github.com/LZhengisme/efficient-attention">[code]</a> <br> 
  <b>Lin Zheng</b>, <a href="https://chongw.github.io">Chong Wang</a>, and <a href="https://ikekonglp.github.io/">Lingpeng Kong</a> <br>
  In International Conference on Machine Learning (ICML), 2022 

- Ripple Attention for Visual Perception with Sub-quadratic Complexity <a href="https://arxiv.org/pdf/2110.02453.pdf">[pdf]</a> <br>
  <b>Lin Zheng</b>, Huijie Pan, and <a href="https://ikekonglp.github.io/">Lingpeng Kong</a> <br>
  In International Conference on Machine Learning (ICML), 2022 

- Cascaded Head-colliding Attention <a href="https://aclanthology.org/2021.acl-long.45.pdf">[pdf]</a> <a href="https://github.com/LZhengisme/CODA">[code]</a> <br> 
  <b>Lin Zheng</b>, <a href="https://lividwo.github.io/zywu.github.io/">Zhiyong Wu</a>, and <a href="https://ikekonglp.github.io/">Lingpeng Kong</a> <br>
  In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2021  

- Generative Semantic Hashing Enhanced via Boltzmann Machines <a href="https://www.aclweb.org/anthology/2020.acl-main.71.pdf">[pdf]</a> <a href="https://github.com/LZhengisme/CorrelatedSemanticHashing">[code]</a> <br> 
  <b>Lin Zheng</b>, <a href="https://cse.sysu.edu.cn/content/3796">Qinliang Su</a>, <a href="https://sites.google.com/view/dinghanshen">Dinghan Shen</a>, and <a href="https://cse.buffalo.edu/~changyou/">Changyou Chen</a> <br> 
  In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), 2020 

## Teaching

- Spring 2022, Spring 2023: TA for <a href="https://nlp.cs.hku.hk/comp3314-spring2023/">COMP3314B Machine Learning</a>

## Professional Services

- Reviewer: ACL 2021, NAACL 2021, ICML 2022-2023, NeurIPS 2022, etc.
